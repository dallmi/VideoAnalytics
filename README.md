# ğŸ“¹ Video Analytics Aggregation - Complete Solution

Komplette LÃ¶sung zur Aggregation von Raw Video Events in aussagekrÃ¤ftige User-Engagement Metriken.

## ğŸ¯ Problem

Raw Video Events kommen als einzelne Rows (play, pause, resume, end) in deinen Databricks Storage Layer. Du brauchst **eine aggregierte Row pro User+Video** mit Metriken wie:
- Total Watch Time
- Watch Percentage
- Completion Status
- Engagement Score

**Beispiel:** Peter schaut Video 1 (5 min LÃ¤nge), pausiert, spult zurÃ¼ck, schaut weiter â†’ Du willst **eine Row** die sagt: "Peter hat 43.3% geschaut (130 von 300 Sekunden)".

## âœ¨ LÃ¶sung

Dieses Repository enthÃ¤lt:
1. **PySpark Script fÃ¼r Databricks** - Aggregiert Raw Events
2. **KQL Queries fÃ¼r Azure Log Analytics** - FÃ¼r manuelle Ad-hoc Analysen
3. **Complete Implementation Guide** - Phase-by-Phase Roadmap
4. **Test Data & Validation** - Example Notebook mit Sample Data

---

## ğŸ“¦ Dateien im Package

```
.
â”œâ”€â”€ README.md                              # Diese Datei
â”œâ”€â”€ quick_reference_guide.md               # Schnell-Referenz
â”œâ”€â”€ executive_summary.md                   # Management Summary
â”‚
â”œâ”€â”€ DATABRICKS (HauptlÃ¶sung)
â”‚   â”œâ”€â”€ databricks_video_aggregation.py    # Main PySpark Script
â”‚   â””â”€â”€ databricks_example_notebook.py     # Beispiel Notebook mit Sample Data
â”‚
â”œâ”€â”€ AZURE LOG ANALYTICS (Alternative)
â”‚   â”œâ”€â”€ video_analytics_kql.md             # KQL Queries fÃ¼r alle Szenarien
â”‚   â””â”€â”€ video_analytics_etl.py             # Python ETL fÃ¼r Azure
â”‚
â””â”€â”€ DOCUMENTATION
    â”œâ”€â”€ implementation_roadmap.md          # Implementierungs-Plan
    â””â”€â”€ test_scenarios.md                  # Test Cases & Validierung
```

---

## ğŸš€ Quick Start (Databricks)

### 1. Upload Script
```bash
# Upload zu Databricks Workspace
# Pfad: /Workspace/Users/<your-email>/video_analytics/
```

### 2. Run in Notebook
```python
%run /Workspace/Users/your-email/video_analytics/databricks_video_aggregation

from databricks_video_aggregation import VideoEngagementAggregator

# Initialize
aggregator = VideoEngagementAggregator(
    spark=spark,
    input_table="your_raw_events_table",          # Deine Raw Events
    output_table="aggregated_user_video_engagement",
    video_metadata_table="video_metadata"         # Optional
)

# Run
result = aggregator.run_aggregation()

# Save
aggregator.save_results(result)
```

### 3. Query Results
```python
# Eine Row pro User+Video
df = spark.table("aggregated_user_video_engagement")

# Peters Engagement fÃ¼r Video 1
df.filter(
    (col("userId") == "peter") & 
    (col("videoId") == "video_001")
).show(vertical=True)
```

---

## ğŸ“Š Output Schema

Die `aggregated_user_video_engagement` Tabelle hat **eine Row pro User+Video** mit:

```
userId                      User Identifier
videoId                     Video Identifier  
videoTitle                  Video Title

-- Kernmetriken
totalWatchTime              Sekunden geschaut (inkl. Replays)
totalUniqueSecondsWatched   Unique Sekunden (ohne Replays doppelt zu zÃ¤hlen)
watchPercentage             % des Videos geschaut
completionPercentage        % des Videos erreicht (max position)

-- Sessions
sessionCount                Anzahl Watch-Sessions
completionCount             Wie oft fertig geschaut

-- Engagement
engagementScore             Gewichteter Score
engagementTier              High / Medium / Low / Minimal

-- Weitere Metriken
avgPausesPerSession, totalForwardSkips, totalBackwardSkips, etc.
```

Siehe `quick_reference_guide.md` fÃ¼r komplettes Schema.

---

## ğŸ“ Beispiel-Output

**Input (Peters Raw Events):**
```
timestamp           | eventName    | currentTime
--------------------|--------------|------------
10:00:00           | video_play   | 0
10:00:30           | video_pause  | 30      â† 30s geschaut
10:00:35           | video_resume | 30
10:02:05           | video_pause  | 120     â† 90s geschaut
10:02:10           | video_resume | 110     â† Skip back
10:02:20           | video_pause  | 120     â† 10s geschaut
```

**Output (Eine aggregierte Row):**
```
userId: peter
videoId: video_001
videoDuration: 300s

totalWatchTime: 130s (30 + 90 + 10)
uniqueSecondsWatched: 120s (0-120 ohne Replay)
watchPercentage: 43.3%
completionPercentage: 40%
sessionCount: 1
engagementTier: Low
```

---

## ğŸ¨ Features

### âœ… Alle Szenarien abgedeckt:
- âœ… Straightforward Play â†’ End
- âœ… Play â†’ Pause â†’ Resume
- âœ… Browser Close (Session Timeout)
- âœ… Skip Forward/Backward
- âœ… Multiple Sessions (Replay Detection)
- âœ… Multiple Videos per Session
- âœ… Video Switching
- âœ… Replay Behavior

### âœ… Data Quality:
- âœ… Jump Detection (filtert unrealistische Skips)
- âœ… Session Timeout Handling
- âœ… Validation Checks (Watch Time â‰¤ Duration, etc.)
- âœ… Quality Flags fÃ¼r problematische Daten

### âœ… Performance:
- âœ… Optimiert fÃ¼r groÃŸe Datasets (100M+ events)
- âœ… Efficient Interval Merging (fÃ¼r Unique Seconds)
- âœ… Caching von Intermediate Results
- âœ… Partitioning Support

---

## ğŸ“š Dokumentation

### FÃ¼r Developers:
- **`quick_reference_guide.md`** - Schnell-Referenz mit allen wichtigen Infos
- **`databricks_example_notebook.py`** - Komplettes Beispiel mit Sample Data
- **`test_scenarios.md`** - Test Cases & Validierung

### FÃ¼r Product/Management:
- **`executive_summary.md`** - Business Case, ROI, Timeline
- **`implementation_roadmap.md`** - Phase-by-Phase Plan (6-8 Wochen)

### FÃ¼r Azure Log Analytics User:
- **`video_analytics_kql.md`** - KQL Queries fÃ¼r alle Szenarien
- **`video_analytics_etl.py`** - Python ETL fÃ¼r Azure

---

## ğŸ”§ Requirements

### Databricks Solution:
- Databricks Runtime 11.3+ (PySpark 3.3+)
- Delta Lake enabled (empfohlen)
- Input Table mit Columns: `timestamp, userId, sessionId, videoId, eventName, currentTime`

### Azure Log Analytics Solution:
- Azure Data Explorer / Log Analytics Workspace
- AppInsights Events mit CustomDimensions

---

## ğŸ“… Production Setup

### Daily Aggregation Job
```python
# Schedule als Databricks Job (tÃ¤glich um 2 Uhr)
from datetime import datetime, timedelta

yesterday = datetime.now() - timedelta(days=1)
start_date = yesterday.replace(hour=0, minute=0, second=0)
end_date = start_date + timedelta(days=1)

result = aggregator.run_aggregation(
    start_date=start_date,
    end_date=end_date
)

aggregator.save_results(result, mode="append")
```

### Data Quality Monitoring
```python
# Run nach jeder Aggregation
quality_metrics = aggregator.generate_summary_stats(result)

# Alert bei Problemen
if quality_metrics['negative_watch_time'] > 0:
    send_alert("Data quality issue detected!")
```

---

## ğŸ¯ Use Cases

### Analytics & Reporting:
```sql
-- Top Videos by Engagement
SELECT videoId, 
       COUNT(DISTINCT userId) as uniqueViewers,
       AVG(watchPercentage) as avgWatchPercentage
FROM aggregated_user_video_engagement
GROUP BY videoId
ORDER BY uniqueViewers DESC;
```

### User Segmentation:
```sql
-- Power Users (High Engagement)
SELECT userId, 
       COUNT(*) as videosWatched,
       AVG(watchPercentage) as avgWatchPercentage
FROM aggregated_user_video_engagement
WHERE engagementTier = 'High'
GROUP BY userId;
```

### Content Optimization:
```sql
-- Drop-off Analysis
SELECT videoId,
       FLOOR(maxPositionReached / 30) * 30 as position,
       COUNT(*) as dropoffCount
FROM aggregated_user_video_engagement
WHERE completionCount = 0
GROUP BY videoId, position;
```

---

## âš¡ Performance

**Benchmarks (Databricks Standard Cluster):**
- 1M events â†’ ~2-3 minutes
- 10M events â†’ ~15-20 minutes  
- 100M events â†’ ~2-3 hours

**Skalierung:**
- Use autoscaling cluster (2-8 workers)
- Partition output table by date
- Incremental processing (tÃ¤glich statt komplett)

---

## ğŸ› Troubleshooting

### "Column not found"
```python
# Check schema
spark.table("raw_video_events").printSchema()
```

### "Out of Memory"
```python
# Disable unique seconds (weniger genau aber schneller)
result = aggregator.run_aggregation(calculate_unique_seconds=False)
```

### Negative Watch Time
```python
# Debug problematic sessions
spark.sql("SELECT * FROM raw_video_events WHERE userId = 'problematic_user'")
```

Siehe `quick_reference_guide.md` fÃ¼r mehr Troubleshooting Tips.

---

## ğŸ¤ Contributing

Feedback und VerbesserungsvorschlÃ¤ge willkommen!

### Bekannte Limitationen:
- Unique Seconds Calculation kann bei sehr langen Videos (>2h) memory-intensive sein
- Browser-Close ohne Event wird via Heuristik erkannt (nicht 100% akkurat)
- Multi-Device Sessions (gleicher User auf mehreren GerÃ¤ten) werden separat getrackt

---

## ğŸ“„ License

MIT License - Free to use and modify

---

## ğŸ“ Learn More

### WeiterfÃ¼hrende Topics:
- A/B Testing Framework fÃ¼r Video-Features
- Predictive Analytics (Completion Wahrscheinlichkeit)
- Real-time Dashboards mit Structured Streaming
- Advanced Segmentation (Cohort Analysis)

---

## ğŸ“ Support

Bei Fragen:
1. Check `quick_reference_guide.md`
2. Review `databricks_example_notebook.py`
3. See `test_scenarios.md` fÃ¼r Beispiele

---

**Happy Analyzing! ğŸš€**

Made with â¤ï¸ for better video analytics
